name: SciComp CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'

jobs:
  quick-validation:
    name: Quick Validation
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8, 3.9, '3.10', 3.11]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run quick deployment check
      run: python scripts/quick_deployment_check.py
    
    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: quick-validation-results-${{ matrix.python-version }}
        path: |
          *.log
          *.json

  comprehensive-tests:
    name: Comprehensive Testing
    runs-on: ubuntu-latest
    needs: quick-validation
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
    
    - name: Run consistency check
      run: python scripts/consistency_check.py
      continue-on-error: true
    
    - name: Run security audit
      run: python scripts/security_audit.py
      continue-on-error: true
    
    - name: Run integration tests
      run: python scripts/integration_tests.py
      timeout-minutes: 10
      continue-on-error: true
    
    - name: Run performance tests
      run: python scripts/performance_regression_tests.py
      timeout-minutes: 15
      continue-on-error: true
    
    - name: Generate test report
      if: always()
      run: |
        python scripts/master_test_runner.py --critical-only --export-report ci_test_report.json
    
    - name: Upload comprehensive test results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: comprehensive-test-results
        path: |
          *.log
          *.json
          deployment_report_*.json
          ci_test_report.json

  cross-platform-test:
    name: Cross-Platform Testing
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ['3.10']
    runs-on: ${{ matrix.os }}
    needs: quick-validation
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Test core functionality
      run: python scripts/validate_framework.py --quick-test
    
    - name: Check platform compatibility
      run: |
        python -c "import sys; print(f'Platform: {sys.platform}')"
        python -c "from pathlib import Path; print(f'Path separator: {Path(\"/\").as_posix()}')"

  documentation-check:
    name: Documentation Check
    runs-on: ubuntu-latest
    needs: quick-validation
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Check README
      run: |
        if [ ! -f "README.md" ]; then
          echo "README.md not found!"
          exit 1
        fi
        
        # Check for required sections
        grep -q "## Installation" README.md || echo "Warning: Installation section missing"
        grep -q "## Quick Start" README.md || echo "Warning: Quick Start section missing"
        grep -q "## Citation" README.md || echo "Warning: Citation section missing"
    
    - name: Check API documentation
      run: |
        doc_count=$(find docs/api -name "*.md" 2>/dev/null | wc -l)
        echo "Found $doc_count API documentation files"
        if [ "$doc_count" -lt 10 ]; then
          echo "Warning: Insufficient API documentation"
        fi
    
    - name: Validate documentation links
      continue-on-error: true
      run: |
        # Simple link checker (would use proper tool in production)
        python -c "
        import re
        with open('README.md') as f:
            content = f.read()
            urls = re.findall(r'https?://[^\s\)]+', content)
            print(f'Found {len(urls)} URLs in README')
            for url in urls[:5]:
                print(f'  - {url}')
        "

  deployment-simulation:
    name: Deployment Simulation
    runs-on: ubuntu-latest
    needs: [comprehensive-tests, cross-platform-test]
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Simulate development deployment
      run: python scripts/deploy.py --env development --dry-run
    
    - name: Simulate staging deployment
      run: python scripts/deploy.py --env staging --dry-run
    
    - name: Simulate production deployment
      run: python scripts/deploy.py --env production --dry-run
    
    - name: Health check simulation
      run: python scripts/monitor.py --once

  notify-results:
    name: Notify Results
    runs-on: ubuntu-latest
    needs: [comprehensive-tests, cross-platform-test, documentation-check, deployment-simulation]
    if: always()
    
    steps:
    - name: Check job statuses
      run: |
        echo "CI/CD Pipeline completed"
        echo "Quick Validation: ${{ needs.quick-validation.result }}"
        echo "Comprehensive Tests: ${{ needs.comprehensive-tests.result }}"
        echo "Cross-Platform: ${{ needs.cross-platform-test.result }}"
        echo "Documentation: ${{ needs.documentation-check.result }}"
        echo "Deployment Sim: ${{ needs.deployment-simulation.result }}"
    
    - name: Set pipeline status
      run: |
        if [ "${{ needs.comprehensive-tests.result }}" = "failure" ] || \
           [ "${{ needs.cross-platform-test.result }}" = "failure" ]; then
          echo "Pipeline failed - critical tests did not pass"
          exit 1
        else
          echo "Pipeline succeeded - all critical tests passed"
        fi